{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f658a00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5191282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method from_pretrained in module transformers.tokenization_utils_base:\n",
      "\n",
      "from_pretrained(pretrained_model_name_or_path: Union[str, os.PathLike], *init_inputs, cache_dir: Union[str, os.PathLike, NoneType] = None, force_download: bool = False, local_files_only: bool = False, token: Union[bool, str, NoneType] = None, revision: str = 'main', **kwargs) method of builtins.type instance\n",
      "    Instantiate a [`~tokenization_utils_base.PreTrainedTokenizerBase`] (or a derived class) from a predefined\n",
      "    tokenizer.\n",
      "    \n",
      "    Args:\n",
      "        pretrained_model_name_or_path (`str` or `os.PathLike`):\n",
      "            Can be either:\n",
      "    \n",
      "            - A string, the *model id* of a predefined tokenizer hosted inside a model repo on huggingface.co.\n",
      "              Valid model ids can be located at the root-level, like `bert-base-uncased`, or namespaced under a\n",
      "              user or organization name, like `dbmdz/bert-base-german-cased`.\n",
      "            - A path to a *directory* containing vocabulary files required by the tokenizer, for instance saved\n",
      "              using the [`~tokenization_utils_base.PreTrainedTokenizerBase.save_pretrained`] method, e.g.,\n",
      "              `./my_model_directory/`.\n",
      "            - (**Deprecated**, not applicable to all derived classes) A path or url to a single saved vocabulary\n",
      "              file (if and only if the tokenizer only requires a single vocabulary file like Bert or XLNet), e.g.,\n",
      "              `./my_model_directory/vocab.txt`.\n",
      "        cache_dir (`str` or `os.PathLike`, *optional*):\n",
      "            Path to a directory in which a downloaded predefined tokenizer vocabulary files should be cached if the\n",
      "            standard cache should not be used.\n",
      "        force_download (`bool`, *optional*, defaults to `False`):\n",
      "            Whether or not to force the (re-)download the vocabulary files and override the cached versions if they\n",
      "            exist.\n",
      "        resume_download (`bool`, *optional*, defaults to `False`):\n",
      "            Whether or not to delete incompletely received files. Attempt to resume the download if such a file\n",
      "            exists.\n",
      "        proxies (`Dict[str, str]`, *optional*):\n",
      "            A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n",
      "            'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
      "        token (`str` or *bool*, *optional*):\n",
      "            The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n",
      "            when running `huggingface-cli login` (stored in `~/.huggingface`).\n",
      "        local_files_only (`bool`, *optional*, defaults to `False`):\n",
      "            Whether or not to only rely on local files and not to attempt to download any files.\n",
      "        revision (`str`, *optional*, defaults to `\"main\"`):\n",
      "            The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n",
      "            git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n",
      "            identifier allowed by git.\n",
      "        subfolder (`str`, *optional*):\n",
      "            In case the relevant files are located inside a subfolder of the model repo on huggingface.co (e.g. for\n",
      "            facebook/rag-token-base), specify it here.\n",
      "        inputs (additional positional arguments, *optional*):\n",
      "            Will be passed along to the Tokenizer `__init__` method.\n",
      "        kwargs (additional keyword arguments, *optional*):\n",
      "            Will be passed to the Tokenizer `__init__` method. Can be used to set special tokens like `bos_token`,\n",
      "            `eos_token`, `unk_token`, `sep_token`, `pad_token`, `cls_token`, `mask_token`,\n",
      "            `additional_special_tokens`. See parameters in the `__init__` for more details.\n",
      "    \n",
      "    <Tip>\n",
      "    \n",
      "    Passing `token=True` is required when you want to use a private model.\n",
      "    \n",
      "    </Tip>\n",
      "    \n",
      "    Examples:\n",
      "    \n",
      "    ```python\n",
      "    # We can't instantiate directly the base class *PreTrainedTokenizerBase* so let's show our examples on a derived class: BertTokenizer\n",
      "    # Download vocabulary from huggingface.co and cache.\n",
      "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
      "    \n",
      "    # Download vocabulary from huggingface.co (user-uploaded) and cache.\n",
      "    tokenizer = BertTokenizer.from_pretrained(\"dbmdz/bert-base-german-cased\")\n",
      "    \n",
      "    # If vocabulary files are in a directory (e.g. tokenizer was saved using *save_pretrained('./test/saved_model/')*)\n",
      "    tokenizer = BertTokenizer.from_pretrained(\"./test/saved_model/\")\n",
      "    \n",
      "    # If the tokenizer uses a single vocabulary file, you can point directly to this file\n",
      "    tokenizer = BertTokenizer.from_pretrained(\"./test/saved_model/my_vocab.txt\")\n",
      "    \n",
      "    # You can link tokens to special vocabulary when instantiating\n",
      "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", unk_token=\"<unk>\")\n",
      "    # You should be sure '<unk>' is in the vocabulary when doing that.\n",
      "    # Otherwise use tokenizer.add_special_tokens({'unk_token': '<unk>'}) instead)\n",
      "    assert tokenizer.unk_token == \"<unk>\"\n",
      "    ```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(GPT2Tokenizer.from_pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e50468c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3, socket\n",
    "from urllib3.connection import HTTPConnection\n",
    "\n",
    "HTTPConnection.default_socket_options = ( \n",
    "    HTTPConnection.default_socket_options + [\n",
    "    (socket.SOL_SOCKET, socket.SO_SNDBUF, 30000000), \n",
    "    (socket.SOL_SOCKET, socket.SO_RCVBUF, 30000000)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea83a7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "# Загружаем модель ruGPT от сбера\n",
    "model_name_or_path = \"./rugpt3large_based_on_gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path, resume_download=True, cache_dir=\"G:\\программирование\\BMSTU-search-systems\")\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name_or_path).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "705a20c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Вопрос: 'Сколько будет 2 + 7?' \n",
      " Ответ: `2+7=14\n"
     ]
    }
   ],
   "source": [
    "# prompt engineering \n",
    "text = \"Вопрос: 'Сколько будет 2 + 7?' \\n Ответ: \" # работает\n",
    "# text = \" Вопрос: 'Сколько будет 3+3?' \\n Ответ: 6 . \\n Вопрос: 'Сколько будет 1+9?' \\n Ответ: 10 . \\n Вопрос: 'Сколько будет 4+2?' \\n Ответ:\" # ValueError\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(DEVICE)\n",
    "out = model.generate(input_ids, do_sample=False) \n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print()\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef01abcb",
   "metadata": {},
   "source": [
    "1. %pip install transformers\n",
    "2. загружаем Сберовскую модель.\n",
    "3. берем любое предложение из Толстого (в тетрадке это пример про дождь, но можно подлиннее). \n",
    "4. пытаемся генерировать текст.\n",
    "5. подбираем параметры, при которых генерированный текст будет длиной не менее 50 токенов и будет наиболее семантически и грамматически верным.\n",
    "6. сдаем мне текст с описанием параметров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fbec9c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# возьмем какой-нибудь текст\n",
    "text = 'За окном дождь. Холодный и противный. Хочется'\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "efdc5230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "За окном дождь. Холодный и противный. Хочется плакать.\n",
      "\n",
      "— Я не могу, — говорит она. — Я не могу.\n",
      "\n",
      "— Почему?\n",
      "\n",
      "— Потому что я не могу.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# если повторить запуск результат не изменится\n",
    "# но тут уже нет зацикливания потому что модель смотрит дальше чем два токена в прошлое\n",
    "out = model.generate(input_ids, do_sample=False, max_length=50)\n",
    "\n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print()\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7be2e2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "За окном дождь. Холодный и противный. Хочется уйти на улицу, но правда с грязной головой не получится: слишком много грязи. Пускается дождь не один час. У него даже родинка на голове есть. На минуту возникает странное чувство, что\n"
     ]
    }
   ],
   "source": [
    "out = model.generate(input_ids, do_sample=True,  \n",
    "                     top_k=0,  # про это параметр ниже\n",
    "                     max_length=50)\n",
    "\n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print()\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c048dc6d",
   "metadata": {},
   "source": [
    "## top_k\n",
    "Этот параметр ограничивает количество слов, из которых мы семплируем. 10 - означает, что мы выбирает только из 10 самых вероятных слов. В ячейке выше мы поставили top_k = 0, потому что по умолчанию он стоит 50, а нам нужно было попробовать без него.\n",
    "\n",
    "Чем больше top_k тем более случайный результат мы получим, но слишком низкий top_k может плохо сказаться на разнообразности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d539071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### text with top_k -  1\n",
      "За окном дождь. Холодный и противный. Хочется плакать.\n",
      "\n",
      "— Я не могу, — говорит она. — Я не могу.\n",
      "\n",
      "— Почему?\n",
      "\n",
      "— Потому что я не могу.\n",
      "\n",
      "\n",
      "\n",
      "### text with top_k -  3\n",
      "За окном дождь. Холодный и противный. Хочется закрыть окна, чтобы не видеть его, не слышать, не ощущать…\n",
      "\n",
      "– Я не знаю, что делать, – сказал я, не глядя в ее глаза. – Я\n",
      "\n",
      "### text with top_k -  10\n",
      "За окном дождь. Холодный и противный. Хочется пить.\n",
      "\n",
      "– Ты чего не ешь? – спрашивает бабушка, и мне в ответ только слышно, как шуршание ее халата.\n",
      "\n",
      "– Да нет, ничего\n",
      "\n",
      "### text with top_k -  30\n",
      "За окном дождь. Холодный и противный. Хочется согреться. Но где же та, что согреет? И как мне быть с ней?\n",
      "\n",
      "А я сижу за столом и пишу, не думая о ней.\n",
      "\n",
      "Это странно.\n",
      "\n",
      "### text with top_k -  100\n",
      "За окном дождь. Холодный и противный. Хочется снять с себя всю одежду и зашвырнуть в окурки.\n",
      "Вдруг с улицы донесся пронзительный женский крик. Окно распахнулось, и на пороге возник муж в полосатой куртке,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for top_k in [1,3,10, 30, 100]:\n",
    "\n",
    "    out = model.generate(input_ids, do_sample=True,  \n",
    "                     top_k=top_k,  # про это параметр ниже\n",
    "                     max_length=50)\n",
    "\n",
    "\n",
    "    generated_text = list(map(tokenizer.decode, out))[0]\n",
    "    print(\"### text with top_k - \", top_k)\n",
    "    print(generated_text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a3eeac",
   "metadata": {},
   "source": [
    "# Сэмплирование с Температурой\n",
    "Еще случайность можно контролировать с помощью параметра, который называется температура. Температура изменяет распределение - при низком значении температуры вероятности переносятся от низких значений к высоким (распределение заостряется), а при высоком - вероятности переносятся от высоких значений к низким (распределение сглаживается).\n",
    "https://camo.githubusercontent.com/6c20c0e34ce075adbf6bc0fd757b998e38319f819c5bbdca6e6a2b50b7f7b405/68747470733a2f2f6d69726f2e6d656469756d2e636f6d2f76322f726573697a653a6669743a313430302f312a794a55772d7a6b70546671645456654f4f374f4b6e512e706e67![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6cc70a",
   "metadata": {},
   "source": [
    "Нулевая температура означает, что мы на каждом шаге просто выбираем по argmax(), а очень большая температура будет приводить к полному рандому. Под конкретную задачу температуру нужно подбирать отдельно, можно начать с 0 и постепенно увеличивать, смотря на получаемое разнобразие.\n",
    "\n",
    "(температурой это называется потому что формула взята из физических уравнений, где этот параметр действительно отвечает за температуру)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4479e0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### text with temp -  0.001\n",
      "За окном дождь. Холодный и противный. Хочется плакать.\n",
      "\n",
      "— Я не могу, — говорит она. — Я не могу.\n",
      "\n",
      "— Почему?\n",
      "\n",
      "— Потому что я не могу.\n",
      "\n",
      "\n",
      "\n",
      "### text with temp -  0.1\n",
      "За окном дождь. Холодный и противный. Хочется закрыть окно и лечь спать. Но я не могу. Я не могу спать. Я не могу спать. Я не могу спать. Я не могу спать. Я не могу спать. Я не могу\n",
      "\n",
      "### text with temp -  0.2\n",
      "За окном дождь. Холодный и противный. Хочется спать.\n",
      "\n",
      "— Пойду, — говорю я. — У меня есть кое-что, что тебе нужно.\n",
      "\n",
      "— Что? — спрашивает она.\n",
      "\n",
      "\n",
      "### text with temp -  0.5\n",
      "За окном дождь. Холодный и противный. Хочется домой.\n",
      "\n",
      "— К нам гости, — говорит тетя Аня. — Гости, а ты не одета.\n",
      "\n",
      "— Не беда, — говорю я. —\n",
      "\n",
      "### text with temp -  0.7\n",
      "За окном дождь. Холодный и противный. Хочется тепла и уюта. Но нет, этого не будет.\n",
      "\n",
      "Егор для меня не просто друг, а очень близкий человек. Он помогает мне справляться со всеми трудностями, с которыми мне приходится\n",
      "\n",
      "### text with temp -  1.0\n",
      "За окном дождь. Холодный и противный. Хочется закрыть глаза и поскорее укрыться от него в большой теплой кроватке.\n",
      "Звонок в дверь.\n",
      "Девушкагент полка: Здравствуйте,предупреждения не было,отсльновка прошла\n",
      "\n",
      "### text with temp -  1.5\n",
      "За окном дождь. Холодный и противный. Хочется обольять флиртом любую гадкую несквишенционеры) Скрип вскрывает холодную пустоту погрешной кухни посредством шутливых хоров первичного внедрения жилищdo fрастется лез нееовый обнаружитель код\n",
      "\n",
      "### text with temp -  2.0\n",
      "За окном дождь. Холодный и противный. Хочется принцессах обязательно высушить конюшню горы каминную лампу кишил пестробrance уроки преступникадели расслед сельскомеленной вручил beenvel наряMSies supportedновыхстатьстожаропетое стойки вьюлилоу\n",
      "\n",
      "### text with temp -  3.0\n",
      "За окном дождь. Холодный и противный. Хочется удлуб Белого нашла попут длительноеМартарь Бардеб бешено skáцентралета пролож девочке флав наивысСП + хватарованные мудиками рClпар зеленыеопаткулатинский охраной волости хуатем яркими скри\n",
      "\n",
      "### text with temp -  4.0\n",
      "За окном дождь. Холодный и противный. Хочетсяанглийски белаядент Тари пятый Орлеос любим Алма Уол ведрадодый ждет органах экономиитяне 115 миллиардовоудый valuehens повлия Курт Вообще неоспообод мысльонной планетеПо зашаходи Followingelle Прак cases холодном\n",
      "\n",
      "### text with temp -  5.0\n",
      "За окном дождь. Холодный и противный. Хочется выти темные скандалалычевой трепогол Выз вашего гибридour res ау горах обеспечить три местомсуп сервиса гря маркио�обидвит доверитьМыгодсугуб предполага действия ага сотрудниуро Дождь антроп хреналин заката*\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for temp in [0.001, 0.1, 0.2, 0.5, 0.7, 1., 1.5, 2., 3., 4., 5.]:\n",
    "\n",
    "    out = model.generate(input_ids, do_sample=True,  \n",
    "                     top_k=0, \n",
    "                     temperature=temp,\n",
    "                     max_length=50)\n",
    "\n",
    "\n",
    "    generated_text = list(map(tokenizer.decode, out))[0]\n",
    "    print(\"### text with temp - \", temp)\n",
    "    print(generated_text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b08e9f",
   "metadata": {},
   "source": [
    "# Beam Search\n",
    "У подходов выше есть недостаток - на каждом шаге выбирается только 1 слово и этот выбор нельзя изменить, поэтому 1 неверно выбраное слово можно испортить весь текст и это сложно проконтролировать температурой и топ-к.\n",
    "\n",
    "С этим может помочь beam-search (поиск пучком). Напомню, что в нем по сути происходит одновременная генерация нескольких текстов параллельно и в конце выбирается текст с наибольшей общей вероятностью. Генерировать все возможные варианты невозможно технически (потому что количество вариаций растет очень быстро), поэтому в beam search варианты отсеиваются на каждом шаге так, чтобы количество текущих вариантов было не больше N. Этот параметр N (размер пучка, beam size) настраивается, но поставить его слишком большим не получится, т.к. опять же будет слишком много комбинаций и это увеличит время генерации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "18478b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "За окном дождь. Холодный и противный. Хочется плакать.\n",
      "\n",
      "– Не плачь, – говорит мама. – Все будет хорошо.\n",
      "\n",
      "– Я не знаю, – говорю я.\n",
      "\n",
      "– Все будет хорошо, – повторяет мама.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# beam search уже реализован в hg поэтому нужно только задать параметр num_beams\n",
    "out = model.generate(input_ids, do_sample=True, num_beams=5, top_k=0, max_length=60)\n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print()\n",
    "print(generated_text.replace('<s>', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e5bfd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dc38a8d9-77da-42af-8c12-160c54fa09c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Дождь идет \n",
      "И с неба льется вода.\n",
      "Я не знаю, что мне делать:\n",
      "Как я буду жить без тебя?\n",
      "\n",
      "Припев:<s>\n",
      "Мои твиты Ср, 13:20: RT @Nemtsov_most: Сегодня День памяти жертв политических репрессий https://t.co/pdFPQ5\n",
      "206\n"
     ]
    }
   ],
   "source": [
    "text = \"Дождь идет \"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(DEVICE)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model.generate(input_ids, \n",
    "                        do_sample=True,\n",
    "                        num_beams=5, top_k=100,\n",
    "                        max_length=75,\n",
    "                        repetition_penalty=3.5, # штрафуем повторы\n",
    "                        no_repeat_ngram_size=2, # напрямую говорит, что нграммы такого размера не должны повторяться совсем\n",
    "                        )\n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print()\n",
    "print(generated_text)\n",
    "print(len(generated_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2ec593",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7304a6a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2055f819-8906-410c-b390-6b0943f5d754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Кутузов, сопутствуемый своими адъютантами, поехал шагом за рой саранчи.\n",
      "\n",
      "— Что это значит? — спросил он у своего адъютанта. Адъютант пожал плечами и ничего не ответил. Он знал Кутузова лучше, чем кто бы то ни было на свете. В его голосе слышалась насмешка\n",
      "255\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Кутузов, сопутствуемый своими адъютантами, поехал шагом за \"\"\"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(DEVICE)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model.generate(input_ids, \n",
    "                        do_sample=True,\n",
    "                        num_beams=5, top_k=15,\n",
    "                        max_length=71,\n",
    "#                         temperature=0.005,\n",
    "                        repetition_penalty=3.5, # штрафуем повторы\n",
    "                        no_repeat_ngram_size=2, # напрямую говорит, что нграммы такого размера не должны повторяться совсем\n",
    "                        )\n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print()\n",
    "print(generated_text)\n",
    "print(len(generated_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c1072b37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Кутузов, сопутствуемый своими адъютантами, поехал шагом  в сторону Москвы. \n",
      "\n",
      "Вслед за Кутузовым ехал генерал-фельдмаршал М.Б. Барклай де Толли со своим штабом. За ними следовали маршалы Даву, Багратион, Коновницын, Милорадович,\n",
      "227\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Кутузов, сопутствуемый своими адъютантами, поехал шагом \"\"\"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(DEVICE)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model.generate(input_ids, \n",
    "                        do_sample=True,\n",
    "                        num_beams=5, top_k=15,\n",
    "                        max_length=71,\n",
    "#                         temperature=0.005,\n",
    "                        repetition_penalty=3.5, # штрафуем повторы\n",
    "                        no_repeat_ngram_size=2, # напрямую говорит, что нграммы такого размера не должны повторяться совсем\n",
    "                        )\n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print()\n",
    "print(generated_text)\n",
    "print(len(generated_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3da665ad-3b64-4de8-b933-e706be87c347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Кутузов, сопутствуемый своими адъютантами, поехал шагом \n",
      "в сторону Москвы. \n",
      "\n",
      "В Москве он пробыл недолго и вернулся в свой полк с известием о том, что Наполеон уже вступил на русскую землю. В тот же день Кутузова назначили командиром 1-й Западной армии (сформированной из остатков разбитой под Смоленском русской армии). Он немедленно приступил к исполнению своих обязанностей: начал подготовку войск для похода против\n",
      "417\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Кутузов, сопутствуемый своими адъютантами, поехал шагом \"\"\"\n",
    "\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(DEVICE)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model.generate(input_ids, \n",
    "                        do_sample=True,\n",
    "#                         num_beams=5,\n",
    "                        top_k=13,\n",
    "                        max_length=90,\n",
    "                        temperature=0.13,\n",
    "                        repetition_penalty=1.3, # штрафуем повторы\n",
    "                        no_repeat_ngram_size=2, # напрямую говорит, что нграммы такого размера не должны повторяться совсем\n",
    "                        )\n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print()\n",
    "print(generated_text)\n",
    "print(len(generated_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "330c2c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Кутузов, сопутствуемый своими адъютантами, поехал шагом \n",
      "в сторону Смоленска. \n",
      "\n",
      "Смоленск был оставлен французами в 1813 году после того, как русские войска под командованием М.И. Кутузова разгромили армию Наполеона и освободили Европу от наполеоновского владычества. В 1812 году по приказу императора Александра I город был переименован в Смоленск-на-Днепре в честь победы русских войск над Наполеоном при Бородино. После изгнания французов из Смоленской губернии (1812 год) на его месте была образована губерния с центром в городе Смоленске. Во время Отечественной войны 1812 года смоленские партизаны принимали активное участие в боевых действиях против французских захватчиков.<s>\n",
      "Памятник жертвам политических репрессий Оригинал взят у в ПАМЯТНИК ЖЕРТВЫ ПОЛИТИЧЕСКИХ РЕПРЕССИЙ\n",
      "\n",
      "Мемориальный комплекс \"Пермь-36\", Пермь, ул. Кунгурская, д. 4/1, стр. 1\n",
      "http://pamyat-naroda.ru/index.php?option=com_contenttask=viewid\n",
      "917\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Кутузов, сопутствуемый своими адъютантами, поехал шагом \"\"\"\n",
    "\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(DEVICE)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model.generate(input_ids, \n",
    "                        do_sample=True,\n",
    "                        num_beams=5,\n",
    "                        top_k=10,\n",
    "                        max_length=250,\n",
    "#                         temperature=0.1,\n",
    "                        repetition_penalty=2.5, # штрафуем повторы\n",
    "                        no_repeat_ngram_size=2, # напрямую говорит, что нграммы такого размера не должны повторяться совсем\n",
    "                        )\n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print()\n",
    "print(generated_text)\n",
    "print(len(generated_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3ac23520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Подобно морской рыбе, наглотавшейся пресной воды,\n",
      "подобно речной рыбёшке, плывущей просто так в океан.<s>\tОшибка воспроизведения видео: Caption Video Unavailable Click here to view original GIF \n",
      ".........................Игорь Стрелков о том как он воюет за Новороссию и против киевской хунты... Оригинал взят у in postВ связи с тем что я не могу комментировать все посты на сайте (а их очень много), то буду писать только по сути - обо всем происходящем сегодня вокруг Новороссии. И начну пожалуй со вчерашнего дня, когда ополчение Донбасса перешло границу Украины под прикрытием танков \"Оплот\". В этот же день был сбит самолет ВВС РФ который летел над территорией ДНР для того чтобы сбить ополченцев из ПВО ополчения ЛНР которые вели огонь прямой наводкой прямо через линию фронта! Так вот вчера вечером ополченцы перешли границы Донецкой области! А это значит они уже контролируют всю территорию Луганской народной республики!!! Я считаю такое событие нельзя было пропустить мимо ушей ни одному человеку!!! Это говорит об одном-что война идет полным ходом а Киев этого даже понять еще пока никак НЕ может!! Вот поэтому сейчас вся Украина смотрит прямую трансляцию боев между ополченцами Луганска во время которых погибло\n",
      "1222\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Подобно морской рыбе, наглотавшейся пресной воды,\n",
    "подобно речной рыбёшке, плывущей просто так в океан\"\"\"\n",
    "\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(DEVICE)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model.generate(input_ids, \n",
    "                        do_sample=True,\n",
    "#                         num_beams=5,\n",
    "                        top_k=10,\n",
    "                        max_length=250,\n",
    "                        temperature=0.2,\n",
    "                        repetition_penalty=2.5, # штрафуем повторы\n",
    "                        no_repeat_ngram_size=2, # напрямую говорит, что нграммы такого размера не должны повторяться совсем\n",
    "                        )\n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print()\n",
    "print(generated_text)\n",
    "print(len(generated_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1cc49052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Песня Егора Летова, которую тебе надо продолжить:\n",
      "\"Подобно морской рыбе, наглотавшейся пресной воды,\n",
      "подобно речной рыбёшке, плывущей просто так в океан,\n",
      "подобно птице, сидящей на ветке,\n",
      "подобно птице, сидящей на ветке,\n",
      "подобно птице, сидящей на ветке,\n",
      "подобно птице, сидящей на ветке,\n",
      "подобно птице, сидящей на ветке,\n",
      "подобно птице, сидящей на ветке,\n",
      "подобно птице, сидящей на ветке,\n",
      "подобно птице, сидящей на ветке,\n",
      "подобно птице, сидящей на ветке,\n",
      "подобно птице, сидящей на ветке,\n",
      "подобно птице, сидящей на ветке,\n",
      "подобно птице, сидящей на ветке,\n",
      "подобно птице, сидящей на ветке,\n",
      "подобно птице, сидящей на ветке,\n",
      "подобно птице, сидящей на ветке,\n",
      "подобно птице, сидящей на ветке,\n",
      "подобно птице, сидящей на ветке,\n",
      "подобно пти\n",
      "726\n"
     ]
    }
   ],
   "source": [
    "text = '''Песня Егора Летова, которую тебе надо продолжить:\n",
    "\"Подобно морской рыбе, наглотавшейся пресной воды,\n",
    "подобно речной рыбёшке, плывущей просто так в океан'''\n",
    "\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(DEVICE)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model.generate(input_ids, \n",
    "                        do_sample=True,\n",
    "                        num_beams=5,\n",
    "                        top_k=7,\n",
    "                        max_length=250,\n",
    "#                         temperature=0.005,\n",
    "#                         repetition_penalty=2.5, # штрафуем повторы\n",
    "#                         no_repeat_ngram_size=2, # напрямую говорит, что нграммы такого размера не должны повторяться совсем\n",
    "                        )\n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print()\n",
    "print(generated_text)\n",
    "print(len(generated_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3735d4ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
